#!/usr/bin/env bash

# identify-conversation-flows - Find concepts that conversations naturally flow toward

set -euo pipefail

# Source configuration and modular libraries
source "${0%/*}/../../.ks-env"
source "$KS_ROOT/lib/core.sh"
source "$KS_ROOT/lib/error.sh"
source "$KS_ROOT/lib/usage.sh"
source "$KS_ROOT/lib/argparse.sh"

# Standardized usage function
usage() {
    declare -a arguments=(
        "EXPERIMENT_NAME          Name of dialogue experiment to analyze"
    )
    declare -a examples=(
        "identify-conversation-flows scientist-philosopher-emergence  # Analyze conversation flows"
        "identify-conversation-flows --min-inflows 5 my-experiment   # Require at least 5 inflows"
        "identify-conversation-flows --format json my-experiment     # Output as JSON"
    )
    ks_generate_usage \
        "Find concepts that conversations naturally flow toward (conceptual attractors)" \
        "identify-conversation-flows" \
        "[options] EXPERIMENT_NAME" \
        "ANALYZE" \
        arguments \
        examples
}

# Parse arguments using category-based system
ks_parse_category_args "ANALYZE" -- "$@"

# Get experiment name from remaining arguments
EXPERIMENT_NAME="${REMAINING_ARGS[0]:-}"
if [[ -z "$EXPERIMENT_NAME" ]]; then
    ks_exit_usage "Experiment name required"
fi

# Validate experiment exists
EXPERIMENT_DIR="$KS_EXPERIMENTS_DIR/$EXPERIMENT_NAME"
if [[ ! -d "$EXPERIMENT_DIR" ]]; then
    ks_exit_error "Experiment directory not found: $EXPERIMENT_DIR"
fi

# Check if knowledge graph exists
KG_DB="$EXPERIMENT_DIR/knowledge/kg.db"
if [[ ! -f "$KG_DB" ]]; then
    ks_exit_error "Knowledge graph database not found: $KG_DB. Run distillation first."
fi

# Set default minimum inflows
MIN_INFLOWS="${MIN_INFLOWS:-3}"

# Analysis functions

identify_flows() {
    local kg_db="$1"
    local min_inflows="$2"
    
    # SQL query to find conceptual attractors based on edge flow patterns
    # This implements the "Conceptual Attractor Identification" query from issue #20
    local query="
    WITH experiment_timerange AS (
        SELECT 
            MIN(c.created) as experiment_start,
            MAX(c.created) as experiment_end
        FROM concepts c
        WHERE c.source_ref IN (
            SELECT event_id FROM event_metadata 
            WHERE key = 'experiment_name' AND value = '$EXPERIMENT_NAME'
        )
    ),
    conversation_flow AS (
        SELECT 
            e.source_id, 
            e.target_id, 
            COUNT(*) as transition_count,
            AVG(e.strength) as avg_strength,
            MIN(e.created) as first_transition,
            MAX(e.created) as last_transition
        FROM edges e
        CROSS JOIN experiment_timerange et
        WHERE e.created BETWEEN et.experiment_start AND et.experiment_end
        GROUP BY e.source_id, e.target_id
    )
    SELECT 
        c.name as attractor_concept,
        SUM(cf.transition_count) as total_inflows,
        AVG(cf.avg_strength) as avg_flow_strength,
        COUNT(DISTINCT cf.source_id) as unique_sources,
        c.weight as concept_weight,
        MIN(cf.first_transition) as first_flow,
        MAX(cf.last_transition) as last_flow,
        GROUP_CONCAT(DISTINCT c_source.name) as source_concepts
    FROM conversation_flow cf
    JOIN concepts c ON cf.target_id = c.id
    LEFT JOIN concepts c_source ON cf.source_id = c_source.id
    GROUP BY c.id, c.name, c.weight
    HAVING total_inflows >= $min_inflows
    ORDER BY total_inflows DESC, avg_flow_strength DESC
    LIMIT 30;
    "
    
    if [[ -n "$VERBOSE" ]]; then
        echo "Executing conversation flow analysis..."
        echo "Minimum inflows required: $min_inflows"
    fi
    
    # Execute query
    sqlite3 "$kg_db" "$query"
}

format_output() {
    local format="${FORMAT:-table}"
    
    case "$format" in
        "json")
            # Convert pipe-separated output to JSON
            awk -F'|' 'BEGIN { print "[" } 
            NR > 1 { 
                if (NR > 2) print ",";
                printf "  {\"attractor_concept\": \"%s\", \"total_inflows\": %d, \"avg_flow_strength\": %.3f, \"unique_sources\": %d, \"concept_weight\": %.3f, \"first_flow\": \"%s\", \"last_flow\": \"%s\", \"source_concepts\": \"%s\"}", 
                $1, $2, $3, $4, $5, $6, $7, $8
            } 
            END { print "\n]" }'
            ;;
        "csv")
            # Convert to CSV format
            sed 's/|/,/g'
            ;;
        "table"|*)
            # Default table format with headers
            {
                echo "ATTRACTOR|INFLOWS|FLOW_STRENGTH|SOURCES|WEIGHT|FIRST_FLOW|LAST_FLOW|SOURCE_CONCEPTS"
                echo "---------|-------|-------------|-------|------|----------|---------|---------------"
                cat
            } | column -t -s '|'
            ;;
    esac
}

# Main execution
main() {
    if [[ -n "$STATUS" ]]; then
        echo "Conversation flow analysis: operational"
        echo "Available experiments:"
        if [[ -d "$KS_EXPERIMENTS_DIR" ]]; then
            find "$KS_EXPERIMENTS_DIR" -name "logex-config.yaml" -exec dirname {} \; | xargs -I {} basename {} | sort
        fi
        exit 0
    fi
    
    if [[ -n "$DRY_RUN" ]]; then
        echo "Would analyze experiment: $EXPERIMENT_NAME"
        echo "Would identify conversation flows with min_inflows >= $MIN_INFLOWS"
        echo "Would format output as: ${FORMAT:-table}"
        exit 0
    fi
    
    # Identify and format flows
    identify_flows "$KG_DB" "$MIN_INFLOWS" | format_output
    
    if [[ -n "$VERBOSE" ]]; then
        echo
        echo "Analysis complete. Concepts with $MIN_INFLOWS+ inbound connections are shown."
        echo "These represent 'conceptual attractors' - ideas that other concepts naturally connect to."
        echo "Higher inflow counts suggest concepts that serve as focal points in the dialogue."
    fi
}

main "$@"